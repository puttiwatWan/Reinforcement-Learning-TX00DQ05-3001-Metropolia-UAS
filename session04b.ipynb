{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TX00DQ05-3001 Exercises 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Common Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "ROWS_COUNT = 4\n",
    "COLUMNS_COUNT = 4\n",
    "TERMINATING = [(0,0), (ROWS_COUNT-1, COLUMNS_COUNT-1)]\n",
    "ACTIONS = ['←','↑','↓','→']\n",
    "\n",
    "alpha = 0.5\n",
    "lamb = 0.5\n",
    "gamma = 1\n",
    "rewards = [-1, -1, -1, -1]\n",
    "\n",
    "def random_state():\n",
    "    row = 0\n",
    "    column = 0\n",
    "    \n",
    "    while (row, column) in TERMINATING:\n",
    "        row = random.randint(0,ROWS_COUNT)\n",
    "        column = random.randint(0,COLUMNS_COUNT)\n",
    "\n",
    "    return (row, column)\n",
    "\n",
    "def random_action(action_prob):\n",
    "    if type(action_prob) is dict:\n",
    "        prob = []\n",
    "        for a in ACTIONS:\n",
    "            prob.append(action_prob[a])\n",
    "    else:\n",
    "        prob = action_prob\n",
    "    selected_action = random.choice(ACTIONS, p=prob)\n",
    "    return selected_action\n",
    "\n",
    "def find_action_prob(Q_state, epsilon):\n",
    "#   Find prob. of actions from Q for randoming action\n",
    "    val_list = []\n",
    "    action_count = len(ACTIONS)\n",
    "    prob = [0,0,0,0]\n",
    "\n",
    "    for a in ACTIONS:\n",
    "        val_list.append(Q_state[a])\n",
    "\n",
    "    best_action_index = val_list.index(max(val_list))\n",
    "    for a in ACTIONS:\n",
    "        i = ACTIONS.index(a)\n",
    "        if i == best_action_index:\n",
    "            prob[i] = 1 - epsilon + (epsilon/action_count)\n",
    "        else:\n",
    "            prob[i] = epsilon/action_count\n",
    "    return prob\n",
    "\n",
    "def init_state_value():\n",
    "    return np.zeros((ROWS_COUNT, COLUMNS_COUNT))\n",
    "\n",
    "def init_action_value(rows, columns):\n",
    "    q = []\n",
    "    for r in range(rows):\n",
    "        q.append([])\n",
    "        for c in range(columns):\n",
    "            temp_dict = {}\n",
    "            for a in ACTIONS:\n",
    "                temp_dict.update({a : 0})\n",
    "            q[r].append(temp_dict)\n",
    "    return q\n",
    "\n",
    "def init_policy():\n",
    "#   Initial policy with all actions with 0.25 prob for all states\n",
    "    policy = []\n",
    "    prob = [0.25, 0.25, 0.25, 0.25]\n",
    "    for r in range(ROWS_COUNT):\n",
    "        policy.append([])\n",
    "        for c in range(COLUMNS_COUNT):\n",
    "            temp_dict = {}\n",
    "            for a in range(len(ACTIONS)):\n",
    "                if (r,c) in TERMINATING:\n",
    "                    p = 0\n",
    "                else:\n",
    "                    p = prob[a]\n",
    "                temp_dict.update({ACTIONS[a] : p})\n",
    "            policy[r].append(temp_dict)\n",
    "    return policy\n",
    "\n",
    "def next_state(action, state):\n",
    "#   Find the next state\n",
    "    row = state[0]\n",
    "    column = state[1]\n",
    "    \n",
    "    if action == ACTIONS[0]:\n",
    "        column -= 1\n",
    "    elif action == ACTIONS[1]:\n",
    "        row -= 1\n",
    "    elif action == ACTIONS[2]:\n",
    "        row += 1\n",
    "    elif action == ACTIONS[3]:\n",
    "        column += 1\n",
    "        \n",
    "    if row >= ROWS_COUNT:\n",
    "        row = ROWS_COUNT - 1\n",
    "    elif row <= 0:\n",
    "        row = 0\n",
    "    if column >= COLUMNS_COUNT:\n",
    "        column = COLUMNS_COUNT - 1\n",
    "    elif column <= 0:\n",
    "        column = 0\n",
    "    \n",
    "    return (row, column)\n",
    "\n",
    "def extract_policy(Q):\n",
    "    policy = []\n",
    "    for r in range(len(Q)):\n",
    "        policy.append([])\n",
    "        for c in range(len(Q[r])):\n",
    "            if (r,c) in TERMINATING:\n",
    "                policy[r].append('-')\n",
    "            else:\n",
    "                val_list = []\n",
    "                for a in ACTIONS:\n",
    "                    val_list.append(Q[r][c][a])\n",
    "                best_action_index = val_list.index(max(val_list))\n",
    "                policy[r].append(ACTIONS[best_action_index])\n",
    "    return policy\n",
    "\n",
    "def print_Q(value_function):\n",
    "#   For better output format\n",
    "    for a in ACTIONS:\n",
    "        print(a)\n",
    "        for r in range(ROWS_COUNT):\n",
    "            print('[',end='')\n",
    "            for c in range(COLUMNS_COUNT):\n",
    "                print(\"{0:.2f}\".format(round(value_function[r][c][a],2)),end=' ')\n",
    "#                 print(value_function[r][c][a],end=' ')\n",
    "            print(']')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: TD(0) value function estimation\n",
    "\n",
    "Implement value function estimation for Sutton & Barto example 4.1 with TD(0) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function_estimation_TD0(policy):\n",
    "    global ROWS_COUNT, COLUMNS_COUNT, TERMINATING\n",
    "    ROWS_COUNT = 4\n",
    "    COLUMNS_COUNT = 4\n",
    "    TERMINATING = [(0,0), (ROWS_COUNT-1, COLUMNS_COUNT-1)]\n",
    "    \n",
    "    V = init_state_value()\n",
    "    \n",
    "    for i in range(100):\n",
    "        S = random_state()    \n",
    "        while not S in TERMINATING:\n",
    "            (row, column) = S\n",
    "            \n",
    "#           Choose A from given policy\n",
    "            A = random_action(policy[row][column])\n",
    "            \n",
    "#           Observe R and S'\n",
    "            R = rewards[ACTIONS.index(A)]\n",
    "            S_next = next_state(A, S)\n",
    "            (row_next, column_next) = S_next\n",
    "            \n",
    "            V[row][column] += alpha * (R + gamma*V[row_next][column_next] - V[row][column])\n",
    "            S = S_next\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -20.53439918, -23.52482805, -23.19499997],\n",
       "       [-17.82451184, -22.79161997, -22.30013459, -16.1889571 ],\n",
       "       [-24.86362554, -22.74832521, -16.94256773,  -7.65735609],\n",
       "       [-15.38941534,  -6.18471302,  -1.50513969,   0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = init_policy()\n",
    "\n",
    "V = value_function_estimation_TD0(policy)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement TD(0) control \n",
    "\n",
    "Solve Sutton & Barto example 4.1 with TD(0) control (Sarsa) algorithm. Apply the algorithm to the windy world of Sutton & Barto example 6.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa():\n",
    "    global ROWS_COUNT, COLUMNS_COUNT, TERMINATING\n",
    "    ROWS_COUNT = 4\n",
    "    COLUMNS_COUNT = 4\n",
    "    TERMINATING = [(0,0), (ROWS_COUNT-1, COLUMNS_COUNT-1)]\n",
    "    \n",
    "    Q = init_action_value(ROWS_COUNT, COLUMNS_COUNT)\n",
    "    \n",
    "#   Episodes loop\n",
    "    for i in range(100):\n",
    "        epsilon = 1/(i+1)\n",
    "        S = random_state()\n",
    "        (row, column) = S\n",
    "        \n",
    "#       Choose A from S by epsilon-greedy\n",
    "        action_prob = find_action_prob(Q[row][column], epsilon)\n",
    "        A = random_action(action_prob)\n",
    "\n",
    "#       Steps loop\n",
    "        while not S in TERMINATING:\n",
    "            (row, column) = S\n",
    "            \n",
    "#           Observe R and S'\n",
    "            R = rewards[ACTIONS.index(A)]\n",
    "            S_next = next_state(A, S)\n",
    "            (row_next, column_next) = S_next\n",
    "            \n",
    "#           Choose A' from S'\n",
    "            action_prob = find_action_prob(Q[row_next][column_next], epsilon)\n",
    "            A_next = random_action(action_prob)\n",
    "            \n",
    "            Q[row][column][A] += alpha * (R + gamma*Q[row_next][column_next][A] - Q[row][column][A])\n",
    "            S = S_next\n",
    "            A = A_next\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "←\n",
      "[0.00 -1.00 -1.99 -2.63 ]\n",
      "[-1.00 -1.99 -2.76 -2.16 ]\n",
      "[-2.50 -2.78 -2.77 -1.84 ]\n",
      "[-3.50 -2.00 -1.62 0.00 ]\n",
      "↑\n",
      "[0.00 -1.50 -2.50 -3.00 ]\n",
      "[-1.00 -2.12 -2.75 -2.25 ]\n",
      "[-2.00 -2.78 -2.25 -1.00 ]\n",
      "[-2.91 -2.44 -1.50 0.00 ]\n",
      "↓\n",
      "[0.00 -1.00 -2.68 -2.53 ]\n",
      "[-2.25 -2.59 -2.71 -2.00 ]\n",
      "[-2.25 -3.03 -2.00 -1.00 ]\n",
      "[-3.00 -2.50 -1.00 0.00 ]\n",
      "→\n",
      "[0.00 -1.25 -2.25 -3.00 ]\n",
      "[-1.56 -2.12 -3.09 -2.50 ]\n",
      "[-2.23 -2.87 -1.99 -1.00 ]\n",
      "[-2.87 -2.00 -1.00 0.00 ]\n"
     ]
    }
   ],
   "source": [
    "Q = sarsa()\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windy World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windy_world():\n",
    "    global ROWS_COUNT, COLUMNS_COUNT, TERMINATING\n",
    "    ROWS_COUNT = 7\n",
    "    COLUMNS_COUNT = 10\n",
    "    TERMINATING = [(3,7)]\n",
    "    \n",
    "    Q = init_action_value(7,10)\n",
    "    wind = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "    \n",
    "#   Episodes loop\n",
    "    for i in range(10000):\n",
    "        epsilon = 1/(i+1)\n",
    "        S = (3,0)\n",
    "#         S = random_state()\n",
    "        (row, column) = S\n",
    "        \n",
    "#       Choose A from S by epsilon-greedy\n",
    "        action_prob = find_action_prob(Q[row][column], epsilon)\n",
    "        A = random_action(action_prob)\n",
    "\n",
    "#       Steps loop\n",
    "        while not S in TERMINATING:\n",
    "            (row, column) = S\n",
    "            \n",
    "#           Observe R and S'\n",
    "            R = rewards[ACTIONS.index(A)]\n",
    "            S_next = next_state(A, S)\n",
    "            (row_next, column_next) = S_next\n",
    "            row_next -= wind[column]\n",
    "            if row_next < 0:\n",
    "                row_next = 0\n",
    "            S_next = (row_next, column_next)\n",
    "            \n",
    "#           Choose A' from S'\n",
    "            action_prob = find_action_prob(Q[row_next][column_next], epsilon)\n",
    "            A_next = random_action(action_prob)\n",
    "            \n",
    "            Q[row][column][A] += alpha * (R + gamma*Q[row_next][column_next][A] - Q[row][column][A])\n",
    "            S = S_next\n",
    "            A = A_next\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "←\n",
      "[-25.00 -25.07 -24.07 -23.25 -23.18 -22.09 -22.60 -21.16 -19.88 -18.80 ]\n",
      "[-26.00 -25.14 -24.19 -24.24 -23.16 -22.93 -20.61 -19.67 -13.37 -10.84 ]\n",
      "[-26.50 -25.54 -24.67 -23.25 -24.29 -21.80 -20.43 -8.45 -12.00 -12.40 ]\n",
      "[-28.00 -27.26 -24.99 -23.85 -23.04 -22.02 -20.31 0.00 -7.31 -8.04 ]\n",
      "[-26.00 -25.00 -24.02 -23.83 -23.30 -21.00 0.00 -5.92 -1.00 -2.00 ]\n",
      "[-25.50 -24.53 -24.42 -23.03 -22.21 0.00 0.00 -5.85 -3.71 -2.85 ]\n",
      "[-24.00 -23.94 -23.85 -22.63 0.00 0.00 0.00 0.00 -3.68 -2.96 ]\n",
      "↑\n",
      "[-25.50 -25.00 -25.00 -32.00 -34.00 -37.50 -31.00 -22.00 -22.00 -11.00 ]\n",
      "[-25.21 -24.64 -24.97 -25.95 -32.97 -37.77 -25.56 -21.55 -17.10 -10.95 ]\n",
      "[-25.96 -25.03 -24.26 -23.82 -34.26 -36.03 -21.00 -13.62 -19.55 -11.16 ]\n",
      "[-26.64 -25.73 -24.76 -25.97 -31.48 -36.56 -30.50 0.00 -6.85 -10.27 ]\n",
      "[-26.37 -25.02 -24.42 -24.39 -23.05 -32.86 0.00 -9.69 -19.36 -10.33 ]\n",
      "[-24.97 -24.27 -23.82 -22.78 -24.68 0.00 0.00 -7.31 -3.93 -4.85 ]\n",
      "[-24.78 -24.21 -23.85 -23.70 0.00 0.00 0.00 0.00 -1.19 -1.63 ]\n",
      "↓\n",
      "[-25.49 -24.48 -23.79 -24.50 -32.50 -35.00 -29.00 -24.00 -25.00 -6.21 ]\n",
      "[-25.33 -25.09 -24.37 -23.50 -23.00 -23.00 -28.16 -21.38 -13.50 -5.21 ]\n",
      "[-26.03 -24.95 -24.11 -23.50 -24.00 -21.50 -23.35 -9.38 -11.00 -4.21 ]\n",
      "[-26.58 -25.68 -24.92 -26.00 -23.00 -21.50 -21.48 0.00 -6.50 -3.21 ]\n",
      "[-25.64 -25.11 -24.07 -24.00 -22.50 -21.50 0.00 0.00 -2.50 -2.21 ]\n",
      "[-24.86 -24.36 -23.92 -23.50 -21.50 0.00 0.00 0.00 -1.00 -1.92 ]\n",
      "[-24.00 -24.50 -23.50 -22.50 0.00 0.00 0.00 0.00 -0.50 -1.50 ]\n",
      "→\n",
      "[-24.86 -24.19 -23.91 -23.35 -22.50 -21.50 -20.50 -19.50 -18.50 -17.50 ]\n",
      "[-25.38 -24.67 -24.10 -23.45 -22.41 -21.50 -20.49 -19.43 -12.75 -10.50 ]\n",
      "[-25.78 -24.97 -24.09 -23.24 -22.50 -21.44 -20.37 -12.34 -10.92 -5.00 ]\n",
      "[-26.50 -25.50 -24.50 -23.50 -22.33 -21.10 -20.21 0.00 -5.87 -4.00 ]\n",
      "[-25.76 -24.88 -24.03 -23.16 -21.91 -20.91 0.00 0.00 -4.41 -2.50 ]\n",
      "[-24.95 -24.37 -23.63 -22.80 -21.63 0.00 0.00 0.00 -1.75 -1.50 ]\n",
      "[-24.32 -23.82 -22.97 -22.31 0.00 0.00 0.00 0.00 0.00 -2.00 ]\n"
     ]
    }
   ],
   "source": [
    "Q = windy_world()\n",
    "print_Q(Q)\n",
    "# print(ACTIONS[3])\n",
    "# Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['→', '→', '↓', '←', '→', '→', '→', '→', '→', '↓'],\n",
       " ['↑', '↑', '→', '→', '→', '→', '→', '→', '→', '↓'],\n",
       " ['→', '↓', '→', '→', '→', '→', '→', '←', '→', '↓'],\n",
       " ['→', '→', '→', '→', '→', '→', '→', '-', '→', '↓'],\n",
       " ['↓', '→', '←', '→', '→', '→', '←', '↓', '←', '←'],\n",
       " ['↓', '↑', '→', '↑', '↓', '←', '←', '↓', '↓', '→'],\n",
       " ['←', '→', '→', '→', '←', '←', '←', '←', '→', '↓']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_policy(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3*: Eligibility trace\n",
    "\n",
    "Do a random walk in example 4.1 gridworld, and create an eligibility trace from the walk.\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "def eligibility_trace():\n",
    "    global ROWS_COUNT, COLUMNS_COUNT, TERMINATING\n",
    "    ROWS_COUNT = 4\n",
    "    COLUMNS_COUNT = 4\n",
    "    TERMINATING = [(0,0), (ROWS_COUNT-1, COLUMNS_COUNT-1)]\n",
    "    \n",
    "    E = init_state_value()\n",
    "    prob = [0.25, 0.25, 0.25, 0.25]\n",
    "    \n",
    "#   Loop for each episode\n",
    "    for i in range(100):\n",
    "        S = random_state()\n",
    "        \n",
    "#       Loop for each step\n",
    "        while not S in TERMINATING:\n",
    "            (row, column) = S\n",
    "            \n",
    "#           Update frequency\n",
    "            E[row][column] += 1\n",
    "    \n",
    "            A = random_action(prob)\n",
    "            S_next = next_state(A, S)\n",
    "            \n",
    "#           Decay for recency\n",
    "            E *= gamma * lamb\n",
    "            S = S_next\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 3.56083790e-66, 2.35098870e-38, 1.41059322e-37],\n",
       "       [4.34243352e-19, 7.81250000e-02, 1.32812500e-01, 1.88170932e-37],\n",
       "       [4.06801701e-06, 3.32107563e-02, 2.54959110e-01, 5.00000000e-01],\n",
       "       [2.59361710e-06, 2.45095334e-04, 6.40876600e-04, 0.00000000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb = 0.5\n",
    "E = eligibility_trace()\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4**: TD(lambda)\n",
    "\n",
    "Implement TD(lambda) algorithm and use it for solving example 6.5. Create a table/plot on the effect of lambda in the performance of the algorithm.\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "def TD_lambda():\n",
    "    global ROWS_COUNT, COLUMNS_COUNT, TERMINATING\n",
    "    ROWS_COUNT = 4\n",
    "    COLUMNS_COUNT = 4\n",
    "    TERMINATING = [(0,0), (ROWS_COUNT-1, COLUMNS_COUNT-1)]\n",
    "    \n",
    "    Q = init_action_value(ROWS_COUNT, COLUMNS_COUNT)\n",
    "\n",
    "#   Loop for each episode\n",
    "    for i in range(100):\n",
    "        epsilon = 1/(i+1)\n",
    "        E = init_action_value(ROWS_COUNT, COLUMNS_COUNT)\n",
    "        S = random_state()\n",
    "        action_prob = [0.25, 0.25, 0.25, 0.25]\n",
    "        A = random_action(action_prob)\n",
    "        \n",
    "#       Loop for each step\n",
    "        while not S in TERMINATING:\n",
    "            (row, column) = S\n",
    "            \n",
    "#           Observe R and S'\n",
    "            R = rewards[ACTIONS.index(A)]\n",
    "            S_next = next_state(A, S)\n",
    "            (row_next, column_next) = S_next\n",
    "            \n",
    "#           Choose A' from S'\n",
    "            action_prob = find_action_prob(Q[row_next][column_next], epsilon)\n",
    "            A_next = random_action(action_prob)\n",
    "            \n",
    "            delta = R + gamma*Q[row_next][column_next][A_next] - Q[row][column][A]\n",
    "            E[row][column][A] += 1\n",
    "            \n",
    "#           For all s,a in all_states,all_actions\n",
    "            for r in range(ROWS_COUNT):\n",
    "                for c in range(COLUMNS_COUNT):\n",
    "                    if not (r,c) in TERMINATING:\n",
    "                        for a in ACTIONS:\n",
    "                            Q[r][c][a] += alpha*delta*E[r][c][a]\n",
    "                            E[r][c][a] *= gamma*lamb\n",
    "            \n",
    "            S = S_next\n",
    "            A = A_next\n",
    "        \n",
    "    return Q        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "←\n",
      "[0.00 -1.00 -2.00 -2.98 ]\n",
      "[-1.90 -2.19 -2.96 -2.83 ]\n",
      "[-3.08 -2.99 -2.87 -2.74 ]\n",
      "[-3.73 -2.71 -2.87 0.00 ]\n",
      "↑\n",
      "[0.00 -1.80 -3.10 -3.77 ]\n",
      "[-1.00 -2.00 -2.95 -3.59 ]\n",
      "[-2.00 -2.88 -2.97 -2.87 ]\n",
      "[-2.98 -3.55 -2.49 0.00 ]\n",
      "↓\n",
      "[0.00 -1.99 -3.77 -2.99 ]\n",
      "[-2.41 -3.02 -2.91 -2.00 ]\n",
      "[-2.76 -2.86 -2.00 -1.00 ]\n",
      "[-3.82 -2.56 -1.92 0.00 ]\n",
      "→\n",
      "[0.00 -2.72 -2.96 -3.57 ]\n",
      "[-2.36 -2.74 -3.00 -2.76 ]\n",
      "[-3.48 -2.91 -2.00 -1.83 ]\n",
      "[-3.03 -2.00 -1.00 0.00 ]\n"
     ]
    }
   ],
   "source": [
    "Q = TD_lambda()\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-', '←', '←', '←'],\n",
       " ['↑', '↑', '↓', '↓'],\n",
       " ['↑', '↓', '→', '↓'],\n",
       " ['↑', '→', '→', '-']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_policy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'←': 0, '↑': 0, '↓': 0, '→': 0},\n",
       "  {'←': -0.9999999850988388,\n",
       "   '↑': -1.8047136068344116,\n",
       "   '↓': -1.9895126972904507,\n",
       "   '→': -2.720859457052739},\n",
       "  {'←': -2.0003060957845293,\n",
       "   '↑': -3.096608159116225,\n",
       "   '↓': -3.772574842654091,\n",
       "   '→': -2.964210197857028},\n",
       "  {'←': -2.984789169489391,\n",
       "   '↑': -3.774473531821924,\n",
       "   '↓': -2.9902209897325065,\n",
       "   '→': -3.5689397230885334}],\n",
       " [{'←': -1.8972083737404368,\n",
       "   '↑': -0.9999961853027344,\n",
       "   '↓': -2.411229545156739,\n",
       "   '→': -2.364628859926597},\n",
       "  {'←': -2.1946728399816493,\n",
       "   '↑': -1.998682456144575,\n",
       "   '↓': -3.0180280804634094,\n",
       "   '→': -2.7448827198531944},\n",
       "  {'←': -2.9632507238058867,\n",
       "   '↑': -2.9505984027989314,\n",
       "   '↓': -2.9145498169236816,\n",
       "   '→': -3.0038076817436377},\n",
       "  {'←': -2.827548989211209,\n",
       "   '↑': -3.589256725491921,\n",
       "   '↓': -1.999977383762598,\n",
       "   '→': -2.755203685577726}],\n",
       " [{'←': -3.083884943274159,\n",
       "   '↑': -2.0001823487703563,\n",
       "   '↓': -2.763261599057305,\n",
       "   '→': -3.481098135250543},\n",
       "  {'←': -2.9910327894779387,\n",
       "   '↑': -2.8813278791603274,\n",
       "   '↓': -2.856913208961487,\n",
       "   '→': -2.9120919621927897},\n",
       "  {'←': -2.868231074884534,\n",
       "   '↑': -2.966700623709926,\n",
       "   '↓': -1.9982664482668042,\n",
       "   '→': -1.9975235174788395},\n",
       "  {'←': -2.7363311323236985,\n",
       "   '↑': -2.874471068382263,\n",
       "   '↓': -0.9999999925494194,\n",
       "   '→': -1.8281249925494194}],\n",
       " [{'←': -3.72593531631707,\n",
       "   '↑': -2.9834911571573732,\n",
       "   '↓': -3.8150077484723997,\n",
       "   '→': -3.0263759199701443},\n",
       "  {'←': -2.7102680113260202,\n",
       "   '↑': -3.5542358595318433,\n",
       "   '↓': -2.560628890991211,\n",
       "   '→': -1.9999845270067453},\n",
       "  {'←': -2.8749895691871643,\n",
       "   '↑': -2.4921476210832907,\n",
       "   '↓': -1.9208984225988388,\n",
       "   '→': -0.9999999981373549},\n",
       "  {'←': 0, '↑': 0, '↓': 0, '→': 0}]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
